{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# load labels and predictions\n",
    "from results import labels, gpt3turbo_0shot, gpt3turbo_0shot_cot, gpt3turbo_5shot, gpt3turbo_5shot_cot, gpt3turbo_10shot, gpt3turbo_10shot_cot, gpt4turbo_0shot, gpt4turbo_0shot_cot, gpt4turbo_5shot, gpt4turbo_5shot_cot, gpt4turbo_10shot, gpt4turbo_10shot_cot, gpt4o_0shot, gpt4o_0shot_cot, gpt4o_5shot, gpt4o_5shot_cot, gpt4o_10shot, gpt4o_10shot_cot\n",
    "# labels\n",
    "# gpt4o_0shot\n",
    "# gpt4o_0shot_cot\n",
    "# gpt4o_5shot\n",
    "# gpt4o_5shot_cot\n",
    "# gpt4o_10shot\n",
    "# gpt4o_10shot_cot\n",
    "\n",
    "# gpt4turbo_0shot\n",
    "# gpt4turbo_0shot_cot\n",
    "# gpt4turbo_5shot\n",
    "# gpt4turbo_5shot_cot\n",
    "# gpt4turbo_10shot\n",
    "# gpt4turbo_10shot_cot\n",
    "\n",
    "# gpt3turbo_0shot\n",
    "# gpt3turbo_0shot_cot\n",
    "# gpt3turbo_5shot\n",
    "# gpt3turbo_5shot_cot\n",
    "# gpt3turbo_10shot\n",
    "# gpt3turbo_10shot_cot\n",
    "\n",
    "models = [gpt3turbo_0shot, gpt3turbo_0shot_cot, gpt3turbo_5shot, gpt3turbo_5shot_cot, gpt3turbo_10shot, gpt3turbo_10shot_cot, gpt4turbo_0shot, gpt4turbo_0shot_cot, gpt4turbo_5shot, gpt4turbo_5shot_cot, gpt4turbo_10shot, gpt4turbo_10shot_cot, gpt4o_0shot, gpt4o_0shot_cot, gpt4o_5shot, gpt4o_5shot_cot, gpt4o_10shot, gpt4o_10shot_cot]\n",
    "\n",
    "\n",
    "\n",
    "# Top-k accuracy\n",
    "\n",
    "## Edit Based Algos\n",
    "### Normalized Hamming Distance\n",
    "from scipy.spatial.distance import hamming\n",
    "# hamming(list(word1),list(word2))\n",
    "\n",
    "### Levenshtein Distance\n",
    "# !pip install levenshtein\n",
    "from Levenshtein import distance as levenshtein\n",
    "# result = distance(\"lewenstein\", \"levenshtein\")\n",
    "\n",
    "### Jaro and Jaro-Winkler Distance\n",
    "from Levenshtein import jaro as jaro\n",
    "# result = jaro(\"lewenstein\", \"levenshtein\")\n",
    "from Levenshtein import jaro_winkler as jaro_winkler\n",
    "# result = jaro_winkler(\"lewenstein\", \"levenshtein\")\n",
    "\n",
    "\n",
    "\n",
    "## Token Based Algos\n",
    "### Jaccard Similarity\n",
    "from sklearn.metrics import jaccard_score as jaccard\n",
    "# jaccard(list(y_true), list(y_pred))\n",
    "\n",
    "### Bleu Score\n",
    "# !pip install tiktoken\n",
    "from evaluate import load\n",
    "import tiktoken\n",
    "bleu = load(\"bleu\")\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "# results = bleu.compute(predictions=predictions, references=references, tokenizer=enc.encode) # tokenizer is optional\n",
    "\n",
    "## Semantic Algos\n",
    "### Bert Score\n",
    "bert = load(\"bertscore\")\n",
    "# predictions = [\"hello there\", \"general kenobi\"]\n",
    "# references = [\"hello there\", \"general kenobi\"]\n",
    "# results = bert.compute(predictions=predictions, references=references, lang=\"en\", model_type=\"distilbert-base-uncased\") # model_type is optional\n",
    "# returns dict: {'precision': [1.0, 1.0], 'recall': [1.0, 1.0], 'f1': [1.0, 1.0], 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.10(hug_trans=4.10.3)'}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tejas/miniconda3/envs/lab/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Accuracy: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Levenshtein Distance: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Jaro Distance: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Jaro-Winkler Distance: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Bleu Score: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "DistilBERT Score: [0.8088757157325744, 0.8174336838722229, 0.82248326420784, 0.8267803645133972, 0.8252770018577575, 0.8269179201126099, 0.8039627623558044, 0.8052772748470306, 0.8269039905071258, 0.8270934987068176, 0.825696837902069, 0.8294089150428772, 0.8329455542564392, 0.8215939688682556, 0.8260041606426239, 0.8292025005817414, 0.842473611831665, 0.8339852464199066]\n",
      "BERT-base Score: [0.6071804565191269, 0.6235128313302993, 0.6358639347553253, 0.6462488996982575, 0.6436516708135605, 0.6491074788570405, 0.602665941119194, 0.5988823908567429, 0.6347126734256744, 0.6416178315877914, 0.6403979873657226, 0.6476112473011016, 0.6565577948093414, 0.6352294367551804, 0.6472495609521866, 0.6475334185361862, 0.6690522360801697, 0.6533367383480072]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "top10_acc = []\n",
    "# hamming_dist = [] # percent, lower better\n",
    "levenshtein_dist = [] # number of edits, lower better\n",
    "jaro_dist = [] # similarity, higher better\n",
    "jaro_winkler_dist = [] # similarity, higher better\n",
    "# jaccard_sim = [] # similarity, higher better\n",
    "bleu_score = [] # sim, higher better\n",
    "distilbert_score = [] # sim, higher better\n",
    "bertbase_score = [] # sim, higher better\n",
    "\n",
    "for model in models:\n",
    "    # each 50-list\n",
    "    acc = []\n",
    "    # ham = []\n",
    "    lev = []\n",
    "    jar = []\n",
    "    jarw = []\n",
    "    # jac = []\n",
    "    ble = []\n",
    "    distilber = []\n",
    "    bertbase = []\n",
    "\n",
    "    for i,top10 in enumerate(model):\n",
    "        # each top-10 predictions\n",
    "        # acc.append(int(labels[i] in top10)) # marks a hit\n",
    "        # # ham.append(min([hamming(list(labels[i]),list(pred)) for pred in top10]))\n",
    "        # lev.append(min([levenshtein(labels[i],pred) for pred in top10]))\n",
    "        # jar.append(max([jaro(labels[i],pred) for pred in top10]))\n",
    "        # jarw.append(max([jaro_winkler(labels[i],pred) for pred in top10]))\n",
    "        # # jac.append(max([jaccard(list(labels[i]), list(pred)) for pred in top10]))\n",
    "        # ble.append(max([bleu.compute(predictions=[pred], references=[labels[i]], tokenizer=enc.encode)[\"bleu\"] for pred in top10]))\n",
    "        distilber.append(max([bert.compute(predictions=[pred], references=[labels[i]], lang=\"en\", model_type=\"distilbert-base-uncased\")[\"f1\"] for pred in top10]))\n",
    "        bertbase.append(max([bert.compute(predictions=[pred], references=[labels[i]], lang=\"en\", model_type=\"bert-base-uncased\")[\"f1\"] for pred in top10]))\n",
    "    \n",
    "    top10_acc.append(np.mean(acc))\n",
    "    # hamming_dist.append(np.mean(ham))\n",
    "    levenshtein_dist.append(np.mean(lev))\n",
    "    jaro_dist.append(np.mean(jar))\n",
    "    jaro_winkler_dist.append(np.mean(jarw))\n",
    "    # jaccard_sim.append(np.mean(jac))\n",
    "    bleu_score.append(np.mean(ble))\n",
    "    distilbert_score.append(np.mean(distilber))\n",
    "    bertbase_score.append(np.mean(bertbase))\n",
    "\n",
    "print(\"Top-10 Accuracy:\", top10_acc)\n",
    "# print(\"Hamming Distance:\", hamming_dist)\n",
    "print(\"Levenshtein Distance:\", levenshtein_dist)\n",
    "print(\"Jaro Distance:\", jaro_dist)\n",
    "print(\"Jaro-Winkler Distance:\", jaro_winkler_dist)\n",
    "# print(\"Jaccard Similarity:\", jaccard_sim)\n",
    "print(\"Bleu Score:\", bleu_score)\n",
    "print(\"DistilBERT Score:\", distilbert_score)\n",
    "print(\"BERT-base Score:\", bertbase_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Top-10 Accuracy: [0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.02, 0.04, 0.02]\n",
    "Levenshtein Distance: [9.88, 9.5, 8.52, 8.52, 8.22, 8.4, 9.18, 9.34, 8.52, 8.48, 8.46, 8.3, 8.96, 8.96, 8.58, 8.24, 7.82, 8.32]\n",
    "Jaro Distance: [0.5767710041076914, 0.5845316525006927, 0.6412084655110972, 0.6335174374129483, 0.6495581272299539, 0.6480516869611297, 0.5912656052524474, 0.5879137164005585, 0.6239030589293747, 0.6242512477775635, 0.6564898178876507, 0.6551056188966404, 0.602555441634389, 0.5966977793162005, 0.6227474981159191, 0.6383363259547471, 0.6823851144269101, 0.6569301790973617]\n",
    "Jaro-Winkler Distance: [0.5781043374410249, 0.5845316525006927, 0.6537481961508277, 0.6444116654071763, 0.6646054034831124, 0.6618361728456155, 0.5949092800961223, 0.5915573912442335, 0.6347380017643175, 0.6384369953633111, 0.673928233176066, 0.6719671592081808, 0.606555441634389, 0.5981200015384226, 0.6331472909157119, 0.6512262952447163, 0.6986541934959891, 0.6665577885249712]\n",
    "Bleu Score: [0.008222672338010394, 0.02, 0.013463642764834973, 0.02866035647786682, 0.03894937534272128, 0.013463642764834973, 0.0, 0.0, 0.05337480609952844, 0.04857151981256029, 0.025359368676124143, 0.03519671371303185, 0.0, 0.015196713713031851, 0.01337480609952844, 0.02, 0.05519671371303185, 0.04857151981256029]\n",
    "DistilBERT Score: [0.8088757157325744, 0.8174336838722229, 0.82248326420784, 0.8267803645133972, 0.8252770018577575, 0.8269179201126099, 0.8039627623558044, 0.8052772748470306, 0.8269039905071258, 0.8270934987068176, 0.825696837902069, 0.8294089150428772, 0.8329455542564392, 0.8215939688682556, 0.8260041606426239, 0.8292025005817414, 0.842473611831665, 0.8339852464199066]\n",
    "BERT-base Score: [0.6071804565191269, 0.6235128313302993, 0.6358639347553253, 0.6462488996982575, 0.6436516708135605, 0.6491074788570405, 0.602665941119194, 0.5988823908567429, 0.6347126734256744, 0.6416178315877914, 0.6403979873657226, 0.6476112473011016, 0.6565577948093414, 0.6352294367551804, 0.6472495609521866, 0.6475334185361862, 0.6690522360801697, 0.6533367383480072]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
